import numpy as np
from scipy.stats import ttest_ind

# Code taken from tutorial values
MU1 = 1
MU2 = 2
SIGMA = 2
alpha = 0.05

# Get random samples, same n
sampSize = 100
n = 1000
pU = np.ones(n)
X1 = np.random.normal(MU1, SIGMA, size=(sampSize, n))
X2 = np.random.normal(MU2, SIGMA, size=(sampSize, n))

# Calculate values from ttest
# The ttest_ind function in scipy returns a tuple of (statistic, pvalue)
# We are only interested in the pvalue, which is the second element
statistic, pU = ttest_ind(X1, X2)

# Determine the proportion of results that fall below desired alpha value
statSigpU = np.sum(pU < alpha) / len(pU) * 100
print(f'percent significant {statSigpU:.2f}% (baseline)')

# Apply the Bonferroni correction
alphaB = alpha / n
statSigpU_B = np.sum(pU < alphaB) / len(pU) * 100
print(f'percent significant {statSigpU_B:.2f}% (Bonferroni)')

# Apply the Benjamini-Hochberg correction
qBH = alpha
sortedpU = np.sort(pU)
critVals = (np.arange(1, len(sortedpU) + 1) / n) * qBH

# Locate the index for the last critical value that is less than or equal
# to its associated ranked p value. Throw sanity check to make sure within
# brackets, then make the new criterion value.
newCritIndex = np.where(sortedpU <= critVals)[0]

if newCritIndex.size == 0:
    # Flags instances when there is no satisfied ranked p less than its
    # associated critical value.
    print('No significant values found')
else:
    alphaBH = sortedpU[newCritIndex[-1]]
    statSigpU_BH = np.sum(pU < alphaBH) / len(pU) * 100
    print(f'percent significant {statSigpU_BH:.2f}% (Benjamini-Hochberg)')
