{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANOVA","provenance":[{"file_id":"1M7xjaMwJUEyULPHfXc3tWG6-WVjCl-uQ","timestamp":1626649097711},{"file_id":"1zwg2aPHoOY9aUMG4h_sm2SNpvCJMKRnK","timestamp":1626385093682},{"file_id":"1OQwscqCNG7QpyTmKaWLafklOUVIcOkW7","timestamp":1626375176143},{"file_id":"11kgk7FpLgbSlA4pjS4cCH1B5mYDUgj5b","timestamp":1626372577818},{"file_id":"1dvkIh9KgmzwwJ7phFBxkHmS3nFTKN_yo","timestamp":1626368223618},{"file_id":"1AmfvDhhfviRQFvONiUVUba_6hof8RDp6","timestamp":1626367834690},{"file_id":"1wTKRgKK5eDUya7FZRHeu1RaoY7kuhiGi","timestamp":1626364730636},{"file_id":"1rdJMusMZDTaM9OGsyt27tCVkSasmRj2O","timestamp":1626357708093},{"file_id":"1HW0L_d5Wpod3jbnY3iG7mLhMG6yWHvF2","timestamp":1626350171621},{"file_id":"1-KxH3FCq5rDyyO33HXxewIv-kKldkINi","timestamp":1626290714843},{"file_id":"14S2ca44h8TKC1hFXjk5ktwBYpGU6R5S-","timestamp":1624411796822}],"collapsed_sections":["pKIiY6p3GRFq","nvmWeSSHSF95","To3hA26GKjv0","k_iBh6ZsLnX7"],"toc_visible":true,"authorship_tag":"ABX9TyNufr0eak85o0VtKRHtyugS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pKIiY6p3GRFq"},"source":["# Definitions"]},{"cell_type":"markdown","metadata":{"id":"x7VmLUr5GTNw"},"source":["When we collect measurements of a variable from 3 or more populations, we need to do multi-sample hypothesis testing: an analysis of variance (ANOVA).\n","\n","Without getting into too many details, it is improper to employ multiple *t*-tests in such a situation. The issue is [multiple comparisons](https://colab.research.google.com/drive/1AmfvDhhfviRQFvONiUVUba_6hof8RDp6?usp=sharing). Let's imagine you want to test the following hypothesis: $H_0:\\mu_1=\\mu_2=\\mu_3$ at $\\alpha=0.05$. For each two-sample *t*-test performed, there is a 95% probability that we will correctly conclude not to reject $H_0$ when the two population means are equal. For the set of 3 hypotheses, the probability of correctly declining to reject all of them is only $0.95*0.95*0.95=0.86$. This means the probability of incorrectly rejecting at least one of the $H_0$s is $1-0.95=0.14$ (or 14%)."]},{"cell_type":"markdown","metadata":{"id":"nvmWeSSHSF95"},"source":["# One-factor ANOVA and the *F* statistic"]},{"cell_type":"markdown","metadata":{"id":"bki88QRVSaj8"},"source":["One-factor (or \"one-way\") Analysis of Variance is used to decide whether two or more groups (or \"levels\", typically denoted as *k*) of a factor have the same mean value. It is a generalization of the [two-sample *t*-test](https://colab.research.google.com/drive/1M7xjaMwJUEyULPHfXc3tWG6-WVjCl-uQ?usp=sharing), which is limited to *k*=2.\n","\n","Specifically, we test the Null hypothesis: \n","\n","$\\quad H_0:\\mu_1=\\mu_2=...\\mu_k$, where *k* is the number of experimental groups. \n","\n","For example, you present 4 different visual stimuli and record the a neuron's firing rate to multiple presentations of each stimulus. Here the \"factor\" is visual stimulus and variable is firing rate. Alternatively, you could take a group of students and randomly assign them to different groups to test the effect of 4 different neuroenhancers on a cognitive test. See [here](https://stats.stackexchange.com/questions/6350/anova-assumption-normality-normal-distribution-of-residuals) for a discussion on the assumptions regarding an ANOVA.\n","\n","We can summarize this four-group example as follows:\n","\n","&nbsp; | **Group 1** | **Group 2** | **Group 3** | **Group 4**\n","-- | -- | -- | -- | -- \n","**Sample Size** | $n_1$ | $n_2$ | $n_3$ | $n_4$\n","**Sample Mean** | $\\bar{X_1}$ | $\\bar{X_2}$ | $\\bar{X_3}$ | $\\bar{X_4}$\n","**Sample Standard Deviation** | $s_1$ | $s_2$ | $s_3$ | $s_4$\n","\n","We assume that $s_1=s_2=s_3=s_4$; that is, the groups might differ in terms of their mean value, but not their standard deviation. \n","\n","The test statistic is the *F* statistic, which is the ratio:\n","\n","$\\quad F=\\frac{explained\\:variance}{unexplained\\:variance}=\\frac{between-group\\:variability}{within-group\\:variability}=\\frac{MSB}{MSE}$\n","\n","This ratio is often summarized in a standard ANOVA table, as follows:\n","\n","*Source of Error* | **DF** | **SS** | **MS** | **F**\n","-- | -- | -- | -- | -- \n","**Between Groups** | *k*-1 | SSB | MSB | $\\frac{MSB}{MSE}$\n","**Error (or Residual)** | *N*-*k* | SSE | MSE | &nbsp;\n","**Total** | *N*-1 | SSR+SSE |  &nbsp; | &nbsp;\n","\n","*N* is the total sample size, equal to $n_1+n_2+n_3+n_4$ in the four-group example above.\n","\n","MSB is the between-group mean square, which quantifies how much variability is accounted for by the groups and is defined as:\n","\n","$\\quad MSB=\\frac{SSB}{df_r}$, where SSB is the between-group sum of squares defined as:\n","\n","$\\quad SSB=\\sum^k_{j=1}{n_j(\\bar{X_j}-\\bar{X})^2}$,\n","\n","where $\\hat{X_j}$ are the per-group means and $\\bar{X}$ is the overall mean, and $df_b=k-1$ is the between-group degrees of freedom.\n","\n","MSE is the mean square error, which quantifies how much variability is left over and is defined as:\n","\n","$\\quad MSE=\\frac{SSE}{df_e}$, where SSE is the error sum of squares, summed across groups, defined as:\n","\n","$\\quad SSE=\\sum^k_{j=1}{\\sum^{n_j}_{i=1}{(X_i-\\bar{X_j})^2}}$,\n","\n","and $df_e=n-k-1$ is the error degrees of freedom \n"]},{"cell_type":"markdown","metadata":{"id":"To3hA26GKjv0"},"source":["# Multiple comparisons for 1-factor ANOVA"]},{"cell_type":"markdown","metadata":{"id":"QtK7ZyfjKojz"},"source":["An ANOVA, if the null hypothesis is rejected, just tells you that the means are different. Which ones are different? Are they all different? Or is just one different from the rest? To answer these questions, you need to do post-hoc multiple comparisons. Unfortunately, there is no one truly accepted method or test. Two very common ones are the Tukey test, which is also known as the \"honestly significance difference\" test, and the Scheffé test.Each test has somewhat different assumptions and is more or less robust to departures from these assumptions.\n","\n","The Scheffé test allows you to test for all combinations of $H_0: \\mu_B - \\mu_A=0$. Also, it is a very easy statistic to compute: \n","\n","if $\\mu_B-\\mu_A > S_{crit}$, then reject the null hypothesis, where\n","\n","$\\quad S_{crit}=\\sqrt{(k-1)\\times F_{alpha, k-1, N-k}}$, *k* is the number of groups, and *N* is the number of groups *x* the number of samples/group (assume equal numbers per gropu).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k_iBh6ZsLnX7"},"source":["# *N*-factor ANOVA"]},{"cell_type":"markdown","metadata":{"id":"clnBO4FU28El"},"source":["One might have different factors or variables for interest in an ANOVA. Let's go back to our idea about neuroenhancers. Imagine, not only that you were interested in whether each neuroenhancer affected the results of an IQ test but you were interested in whether the amount of sleep on the previous night affected the results. So, now there would be two factors: sleep ('a lot' versus 'little') and neuroenhancer (each one). The number of  levels in each factor would be 2 ('a lot' versus 'little') for the sleep factor and the number of tested neuroenhancers for that factor. These can be the same or can be different: you could have 2 sleep levels and 4 different neuroenhancer levels.\n","\n","Now, we have 3 different null hypotheses to test: each factor (or main effect) and the interaction between the two main effects. The first $H_0$ is that there is no effect of sleep on the mean result of the IQ test. The second $H_0$ is that there is no effect of neuroenhancer on the mean result of the IQ test. These two are the hypotheses for each main effect. The interaction $H_0$ is there there is no interaction of sleep and neuroenhancer on the mean effect of the cognitive test.\n","\n","Another important two-factor ANOVA is a repeated measure. Here, we would ask participants to try each neuroenhancer and then take the cognitive test. But we have only one $H_0$: the mean test score is the same for all participants all 3 drugs."]},{"cell_type":"markdown","metadata":{"id":"UqoNXyuxP-go"},"source":["# Additional Resources\n"]},{"cell_type":"markdown","metadata":{"id":"g3rMlGssifYX"},"source":["Using ANOVAs in [Matlab](https://www.mathworks.com/help/stats/analysis-of-variance-anova-1.html), [R](https://www.scribbr.com/statistics/anova-in-r/), and [Python](https://www.marsja.se/four-ways-to-conduct-one-way-anovas-using-python/)."]},{"cell_type":"markdown","metadata":{"id":"tteEm2Qlgbb3"},"source":["# Credits\n","\n","Copyright 2021 by Joshua I. Gold, University of Pennsylvania"]}]}