{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PennNGG/Quantitative-Neurocience/blob/master/Machine%20Learning/Python/Basics%5fof%5fAutoML%5fWith%5fTPOT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjC2L0EMS1Lw"
      },
      "source": [
        "# Tutorial: Basics of AutoML With TPOT \n",
        "\n",
        "\n",
        "__Content creator:__ Diego G. DÃ¡vila (Penn NGG)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncD3o309S_Aa"
      },
      "source": [
        "---\n",
        "# Objectives\n",
        "\n",
        "In this tutorial, you will be introduced to the fundamentals of Automated Machine Learning (AutoML), and learn to apply it to classification and regression problems with TPOT (python package). \n",
        "\n",
        "\n",
        "By the end of this tutorial you will be able to:\n",
        "*   Understand the basics of AutoML\n",
        "*   Develop an intuition for how AutoML can be applied.\n",
        "*   Apply AutoML to develop classification and regression pipelines. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVE7Mrr2TjF_"
      },
      "source": [
        "---\n",
        "#What is Automated Machine Learning?\n",
        "\n",
        "Machine Learning (ML) methods encompass a large landscape of incredibly powerful tools to explore and understand data. However, the size and intricacy of that landscape often precludes or overcomplicates the application of ML by scientists not specializing in AI and informatics. Even within the field of neuroinformatics, the problems of model and parameter selection, and pipeline construction are topics of intense discussion and are subejct to arbitraty choice. \n",
        "\n",
        "AutoML solves these issues by automating the process of applying machine learning to a dataset, from start to finish. This means that the construction of every step of an ML pipeline, from raw data to a model, is fully automated and optimized. \n",
        "\n",
        "In a basic sense, AutoML works by trying multiple combinations of pipelines and parameters over runtime, and comparing the performance of each combination. The best performing pipeline across the runtime is then selected and returned to the human user, along with performance metrics. \n",
        "\n",
        "Different implementations of AutoML work slightly differently. For this tutorial we will focus on [TPOT](http://epistasislab.github.io/tpot/), which uses genetic programming to construct and identify the optimal machine learning pipeline for classification or regression. \n",
        "\n",
        "This high degree of automation is advantageous in that: \n",
        "1. It makes advanced machine learning methods available to non-experts or scientists outside the field of ML/AI.\n",
        "2. Speeds up the time-to-application, and frees up cognitive resources to be focused on scientific questions. \n",
        "3. Given sufficient runtime, results in pipelines that typically outperform human-designed ones. \n",
        "\n",
        "Some disadvantages of AutoML include:\n",
        "1. It is a computationally intensive process. Because AutoML tries many combinations of models, processing steps, and parameters, until an optimal solution is found, days to weeks are required for AutoML processes to run (and powerful computational resources are required).  \n",
        "2. Some implementations of AutoML can be \"black boxes\". In this introduction we will be using TPOT, which outputs the final pipeline as a python script, allowing for a good amount of introspection.\n",
        "3. Due to inherent stochasticity, AutoML could produce multiple solutions to the same problem. This is especially the case for especially complex datasets, or if very little runtime is allowed to test solutions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbERc-jLKzBD"
      },
      "source": [
        "---\n",
        "#Classification Example\n",
        "\n",
        "TPOT is extremely flexible and easy to implement. So long as we have Input and Output training data, we can quickly set up an AutoML process to generate a pipeline. \n",
        "\n",
        "As a first example, we will ask TPOT to develope an optimal solution to a classification problem. In this case, classifying breast cancer. We will make use of the built in breast cancer dataset from scikit-learn to contruct an accurate classification pipeline with AutoML in a few lines of code. \n",
        "\n",
        "Note: While we typically would let TPOT run for several days, trying many different generations. For illustrative purposes we will only let it run for 10 minutes here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCb9ApKcMht9"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw1ZUI1ELjIU"
      },
      "outputs": [],
      "source": [
        "#@title Import Statements\n",
        "import os\n",
        "os.system('pip install tpot')\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_97k2ySLTa"
      },
      "outputs": [],
      "source": [
        "#@title Figure settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_OpF3tbbn3f"
      },
      "source": [
        "---\n",
        "# Create Classification Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ8oeb5jMzWl"
      },
      "outputs": [],
      "source": [
        "# read in the dataset\n",
        "data = load_breast_cancer(as_frame=False) \n",
        "\n",
        "# Split the data up into training and test input/output data. \n",
        "# We will use scikit-learn's train_test_split function to use 75% of the data as training, and 25% as test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['data'].astype(np.float64),\n",
        "    data['target'].astype(np.float64), train_size=0.75, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create a classification pipeline optimizer object\n",
        "# max_time_mins determines how long AutoML tests different pipelines - typically, for an optimal solution, we would let it run for days. \n",
        "# However, for illustrative purposes, we will only let it run fot ten minutes.\n",
        "# Random state is the seed for the first generation. \n",
        "optimizer = TPOTClassifier(max_time_mins=10, verbosity=2, random_state=137) \n",
        "\n",
        "# do the optimization\n",
        "optimizer.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEhRW0HwO-ca"
      },
      "source": [
        "Now, at this point, we already have an optimized pipeline. We can export it to a python script to evaluate and use with the following line:\n",
        "\n",
        "Note: Because we are on colab notebook (on Google's server), we cannot see this script. However, were this running on, say, a jupyter notebook, we could dissect the pipeline fully, line by line. It might be a good exercise after going over this tutorial to try this out on your own machine. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMvPFnUDPG1c"
      },
      "outputs": [],
      "source": [
        "# export optimized pipeline \n",
        "optimizer.export('optimizedTPOTClassificationPipeline.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxwa2NwfSXmt"
      },
      "source": [
        "Great! In a few short lines we've created a highly accurate classifier. We can do many things with this now. If we wanted, we could list all the variables with their associated importance. Each optimal classifier will be different, so we will not illustrate that in this notebook, but we can extract the classifier with the following lines of code to explore. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q0jydAbSkkY"
      },
      "outputs": [],
      "source": [
        "exctracted_best_model = optimizer.fitted_pipeline_.steps[-1][1] # extract the classifier\n",
        "exctracted_best_model.fit(data['data'].astype(np.float64), data['target'].astype(np.float64)) # fit to the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpGzoOMUUhay"
      },
      "source": [
        "---\n",
        "#Regression Example\n",
        "\n",
        "Now that we've built a classification pipeline, let's build a regression pipeline in a similar fashion. In this exercise, we will use scikit-learn's diabetes dataset to build a regressor that can serve as a predictive model. Note that this process is nearly identical to setting up the classification pipeline, reflecting TPOT's ease of use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAMGP18Xbif3"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2Ek7bG8VDl3"
      },
      "outputs": [],
      "source": [
        "#@title Import Statements\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGA4dwFqbs3I"
      },
      "source": [
        "---\n",
        "# Create Regression Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b6eUkinVOon"
      },
      "outputs": [],
      "source": [
        "# read in the dataset\n",
        "data = load_diabetes(as_frame=False) \n",
        "\n",
        "# Split the data up into training and test input/output data into 75% training, 25% test. \n",
        "X_train, X_test, y_train, y_test = train_test_split(data['data'].astype(np.float64),\n",
        "    data['target'].astype(np.float64), train_size=0.75, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create a regression pipeline optimizer object\n",
        "optimizer = TPOTRegressor(max_time_mins=10, verbosity=2, random_state=137) \n",
        "\n",
        "# do the optimization\n",
        "optimizer.fit(X_train, y_train)\n",
        "print(optimizer.score(X_test, y_test)) # report performance "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WItcLRzyaxhH"
      },
      "source": [
        "Ok good, we've created a workable regression pipeline. We can however see that the performance of the \"optimal\" pipeline is somewhat terrible. This is because we only let TPOT run for 10 minutes. As stated earlier, one disadvantage of AutoML solutions is that they often require long runtimes to achieve workable solutions. Outside of this notebook, we could let this run for a few days and reach a best solution that has significantly better performance.\n",
        "\n",
        "Like in the earlier example, we can export our pipeline as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkkX_YyBXsPm"
      },
      "outputs": [],
      "source": [
        "# export optimized pipeline \n",
        "optimizer.export('optimizedTPOTRegressionPipeline.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fsst9WhcDa4"
      },
      "source": [
        "---\n",
        "# Additional Reading\n",
        "\n",
        "The original TPOT paper can be found [here](http://proceedings.mlr.press/v64/olson_tpot_2016.html).\n",
        "\n",
        "A second paper evaluating TPOT can also be found [here](https://dl.acm.org/doi/10.1145/2908812.2908918).\n",
        "\n",
        "Another popular platform for applying AutoML is [auto-sklearn](https://automl.github.io/auto-sklearn/master/index.html#), though as of yet, it can only run on linux systems. \n",
        "\n",
        "MATLAB also has [AutoML capabilities](https://www.mathworks.com/discovery/automl.html). [Here](https://www.mathworks.com/videos/automated-machine-learning-automl-with-matlab-1597226741441.html) is a video explaining how a classification pipeline can be constructed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJprt6EechM"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "What have we learned?\n",
        "* What AutoML is\n",
        "\n",
        "* The basics of AutoML and TPOT\n",
        "\n",
        "* The pros and cons of AutoML \n",
        "\n",
        "* How to apply TPOT to develop an optimized classification pipeline\n",
        "\n",
        "* How to apply TPOT to develop an optimized regression pipeline"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CCb9ApKcMht9",
        "mAMGP18Xbif3"
      ],
      "name": "Basics of AutoML With TPOT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
